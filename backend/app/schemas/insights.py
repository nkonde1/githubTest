"""
AI-powered insights and analytics schemas for the finance platform.

This module defines schemas for:
- AI-generated business insights
- Financial analytics and metrics
- Predictive analytics and forecasting
- ML-powered recommendations
- Real-time monitoring and alerts
"""

from datetime import datetime, date
from typing import Optional, List, Dict, Any, Union, Annotated # Import Annotated
from enum import Enum
from pydantic import BaseModel, Field, validator, model_validator
# No longer need to import constr, confloat, conint directly if using Annotated and Field
# from pydantic.types import confloat, conint, constr # REMOVE THIS LINE
import json


# --- Enums ---

class FocusArea(str, Enum):
    revenue = "revenue"
    cashflow = "cashflow"
    customer_behavior = "customer_behavior"
    inventory = "inventory"
    financing = "financing"
    growth = "growth"
    risk_analysis = "risk_analysis"
    market_trends = "market_trends"

# --- Request Schemas ---

class InsightRequest(BaseModel):
    focus_area: FocusArea = Field(description="Area of business to focus insights on")
    date_range: Optional[int] = Field(30, description="Number of days to analyze")
    parameters: Optional[Dict[str, Any]] = Field(default_factory=dict)
    async_processing: Optional[bool] = Field(False, description="Process asynchronously")

class AnalyticsInsightRequest(BaseModel):
    focus_areas: Optional[List[FocusArea]] = Field(None, description="Areas to analyze")
    date_range: Optional[int] = Field(30, description="Number of days to analyze")
    parameters: Optional[Dict[str, Any]] = Field(default_factory=dict)

class ChatMessage(BaseModel):
    conversation_id: str = Field(description="Conversation session ID")
    content: str = Field(description="User message content")

# --- Response Schemas ---

class InsightTask(BaseModel):
    task_id: str
    status: str
    message: Optional[str] = None
    estimated_completion: Optional[datetime] = None

class InsightResponse(BaseModel):
    task_id: Optional[str] = None
    status: Optional[str] = None
    message: Optional[str] = None
    insights: Optional[Dict[str, Any]] = None
    generated_at: Optional[datetime] = None
    confidence_score: Optional[float] = None
    data_points_analyzed: Optional[int] = None
    estimated_completion: Optional[datetime] = None

class ChatResponse(BaseModel):
    conversation_id: str
    response: str
    suggestions: Optional[List[str]] = None
    data_visualizations: Optional[List[Dict[str, Any]]] = None
    follow_up_questions: Optional[List[str]] = None
    confidence: Optional[float] = None
    response_time: Optional[float] = None

class RecommendationResponse(BaseModel):
    id: str
    title: str
    description: str
    category: str
    priority: str
    impact_score: float
    effort_required: str
    implementation_steps: List[str]
    expected_outcome: str
    confidence: float
    supporting_data: Optional[Dict[str, Any]] = None
    created_at: datetime

# --- Example for AnalyticsInsightRequest (if needed) ---

class AnalyticsInsightRequest(BaseModel):
    focus_areas: Optional[List[FocusArea]] = Field(None, description="Areas to analyze")
    date_range: Optional[int] = Field(30, description="Number of days to analyze")
    parameters: Optional[Dict[str, Any]] = Field(default_factory=dict)

class InsightType(str, Enum):
    """Types of insights generated by the AI system."""
    FINANCIAL_HEALTH = "financial_health"
    REVENUE_TREND = "revenue_trend"
    CASH_FLOW = "cash_flow"
    EXPENSE_OPTIMIZATION = "expense_optimization"
    FRAUD_DETECTION = "fraud_detection"
    CUSTOMER_BEHAVIOR = "customer_behavior"
    SEASONAL_PATTERN = "seasonal_pattern"
    PREDICTIVE_FORECAST = "predictive_forecast"
    FINANCING_OPPORTUNITY = "financing_opportunity"
    RISK_ASSESSMENT = "risk_assessment"
    PERFORMANCE_BENCHMARK = "performance_benchmark"
    OPERATIONAL_EFFICIENCY = "operational_efficiency"


class InsightSeverity(str, Enum):
    """Severity levels for insights and alerts."""
    INFO = "info"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class InsightCategory(str, Enum):
    """High-level categorization of insights."""
    ANALYTICS = "analytics"
    RECOMMENDATIONS = "recommendations"
    ALERTS = "alerts"
    FORECASTS = "forecasts"
    BENCHMARKS = "benchmarks"


class DataSource(str, Enum):
    """Data sources for insight generation."""
    TRANSACTIONS = "transactions"
    ACCOUNTING = "accounting"
    PAYMENTS = "payments"
    INVENTORY = "inventory"
    CUSTOMERS = "customers"
    EXTERNAL_MARKET = "external_market"
    INDUSTRY_BENCHMARK = "industry_benchmark"


class TimeFrame(str, Enum):
    """Time frame for analytics and insights."""
    REAL_TIME = "real_time"
    DAILY = "daily"
    WEEKLY = "weekly"
    MONTHLY = "monthly"
    QUARTERLY = "quarterly"
    YEARLY = "yearly"
    CUSTOM = "custom"


class MetricType(str, Enum):
    """Types of financial metrics."""
    REVENUE = "revenue"
    PROFIT = "profit"
    CASH_FLOW = "cash_flow"
    EXPENSES = "expenses"
    GROWTH_RATE = "growth_rate"
    MARGIN = "margin"
    RATIO = "ratio"
    COUNT = "count"
    PERCENTAGE = "percentage"


class AIModel(str, Enum):
    """AI models used for insight generation."""
    LLAMA_3_2 = "llama_3_2"
    FINANCIAL_ANALYZER = "financial_analyzer"
    FRAUD_DETECTOR = "fraud_detECTOR"
    FORECASTING_MODEL = "forecasting_model"
    RISK_ASSESSOR = "risk_assessor"


class MetricValue(BaseModel):
    """Individual metric value with metadata."""
    name: str = Field(description="Metric name")
    value: Union[float, int, str] = Field(description="Metric value")
    unit: Optional[str] = Field(None, description="Unit of measurement")
    currency: Optional[str] = Field(None, description="Currency for monetary values")
    formatted_value: Optional[str] = Field(None, description="Human-readable formatted value")
    change_percentage: Optional[float] = Field(None, description="Percentage change from previous period")
    trend: Optional[str] = Field(None, pattern="^(up|down|stable|unknown)$", description="Trend direction")
    
    @validator('change_percentage')
    def validate_change_percentage(cls, v):
        if v is not None and abs(v) > 10000:  # Sanity check for extreme values
            raise ValueError('Change percentage seems unrealistic')
        return v


class DataPoint(BaseModel):
    """Individual data point for time series and charts."""
    timestamp: datetime = Field(description="Data point timestamp")
    value: Union[float, int] = Field(description="Data point value")
    label: Optional[str] = Field(None, description="Data point label")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }


class TimeSeriesData(BaseModel):
    """Time series data for charts and trend analysis."""
    metric_name: str = Field(description="Name of the metric")
    time_frame: TimeFrame = Field(description="Time frame of the data")
    data_points: List[DataPoint] = Field(description="List of data points")
    total_count: int = Field(description="Total number of data points")
    start_date: datetime = Field(description="Start date of the series")
    end_date: datetime = Field(description="End date of the series")
    
    @validator('data_points')
    def validate_data_points_order(cls, v):
        """Ensure data points are in chronological order."""
        if len(v) > 1:
            timestamps = [dp.timestamp for dp in v]
            if timestamps != sorted(timestamps):
                raise ValueError('Data points must be in chronological order')
        return v


class InsightActionItem(BaseModel):
    """Actionable recommendation from AI insights."""
    title: Annotated[str, Field(max_length=200, description="Action item title")]
    description: Annotated[str, Field(max_length=1000, description="Detailed description")]
    priority: InsightSeverity = Field(description="Priority level")
    estimated_impact: Optional[str] = Field(None, description="Estimated business impact")
    effort_required: Optional[Annotated[str, Field(pattern="^(low|medium|high)$")]] = Field(
        None, 
        description="Effort required to implement"
    )
    deadline: Optional[date] = Field(None, description="Suggested deadline")
    category: Annotated[str, Field(max_length=100, description="Action category")]
    resources_needed: Optional[List[str]] = Field(None, description="Required resources")
    success_metrics: Optional[List[str]] = Field(None, description="Success measurement criteria")


class AIInsightBase(BaseModel):
    """Base schema for AI-generated insights."""
    # Corrected: Use Annotated[str, Field(strip_whitespace=True, min_length=1, max_length=200)]
    title: Annotated[str, Field(strip_whitespace=True, min_length=1, max_length=200)]
    # Corrected: Use Annotated[str, Field(strip_whitespace=True, min_length=1, max_length=500)]
    summary: Annotated[str, Field(strip_whitespace=True, min_length=1, max_length=500)]
    description: Annotated[str, Field(max_length=2000, description="Detailed insight description")]
    insight_type: InsightType
    category: InsightCategory
    severity: InsightSeverity = Field(default=InsightSeverity.INFO)
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    confidence_score: Annotated[float, Field(ge=0.0, le=1.0, description="AI confidence in the insight")]
    data_sources: List[DataSource] = Field(description="Data sources used")
    time_frame: TimeFrame = Field(description="Time frame analyzed")
    ai_model: AIModel = Field(description="AI model used for generation")


class InsightCreate(AIInsightBase):
    """Schema for creating new insights."""
    user_id: int = Field(description="User ID for the insight")
    business_context: Optional[Dict[str, Any]] = Field(None, description="Business context data")
    raw_data: Optional[Dict[str, Any]] = Field(None, description="Raw data used for analysis")
    processing_metadata: Optional[Dict[str, Any]] = Field(None, description="Processing metadata")


class InsightUpdate(BaseModel):
    """Schema for updating existing insights."""
    title: Optional[Annotated[str, Field(max_length=200)]] = None
    summary: Optional[Annotated[str, Field(max_length=500)]] = None
    description: Optional[Annotated[str, Field(max_length=2000)]] = None
    severity: Optional[InsightSeverity] = None
    is_read: Optional[bool] = None
    is_archived: Optional[bool] = None
    user_feedback: Optional[Annotated[str, Field(max_length=1000)]] = None
    # Corrected: Use Annotated[int, Field(ge=1, le=5)]
    feedback_rating: Optional[Annotated[int, Field(ge=1, le=5)]] = None


class InsightInDB(AIInsightBase):
    """Schema for insights stored in database."""
    id: int = Field(description="Insight unique identifier")
    user_id: int = Field(description="Associated user ID")
    created_at: datetime = Field(description="Creation timestamp")
    updated_at: datetime = Field(description="Last update timestamp")
    is_read: bool = Field(default=False, description="User has read the insight")
    is_archived: bool = Field(default=False, description="Insight is archived")
    read_at: Optional[datetime] = Field(None, description="Timestamp when insight was read")
    
    # Analytics and engagement
    view_count: int = Field(default=0, description="Number of views")
    action_taken: bool = Field(default=False, description="User took action based on insight")
    user_feedback: Optional[str] = Field(None, description="User feedback on insight")
    # Corrected: Use Annotated[int, Field(ge=1, le=5)]
    feedback_rating: Optional[Annotated[int, Field(ge=1, le=5)]] = None
    
    # Related data
    metrics: Optional[List[MetricValue]] = Field(None, description="Associated metrics")
    time_series: Optional[List[TimeSeriesData]] = Field(None, description="Time series data")
    action_items: Optional[List[InsightActionItem]] = Field(None, description="Recommended actions")
    
    class Config:
        from_attributes = True
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }


class AnalyticsQuery(BaseModel):
    """Schema for analytics queries to the AI system."""
    query_text: Annotated[str, Field(max_length=1000, description="Natural language query")]
    time_frame: TimeFrame = Field(default=TimeFrame.MONTHLY)
    start_date: Optional[date] = None
    end_date: Optional[date] = None
    metrics: Optional[List[str]] = Field(None, description="Specific metrics to analyze")
    filters: Optional[Dict[str, Any]] = Field(None, description="Query filters")
    context: Optional[Dict[str, Any]] = Field(None, description="Additional context")
    
    @model_validator(mode='after')
    def validate_date_range(cls, values):
        """Validate date range for custom time frames."""
        time_frame = values.get('time_frame')
        start_date = values.get('start_date')
        end_date = values.get('end_date')
        
        if time_frame == TimeFrame.CUSTOM:
            if not start_date or not end_date:
                raise ValueError('Custom time frame requires start_date and end_date')
            if start_date >= end_date:
                raise ValueError('start_date must be before end_date')
        
        return values


class AnalyticsResponse(BaseModel):
    """Response schema for analytics queries."""
    query_id: str = Field(description="Unique query identifier")
    original_query: Annotated[str, Field(description="Original user query")]
    ai_response: Annotated[str, Field(description="AI-generated response")]
    insights: List[InsightInDB] = Field(description="Generated insights")
    metrics: List[MetricValue] = Field(description="Calculated metrics")
    time_series: Optional[List[TimeSeriesData]] = Field(None, description="Time series data")
    charts_data: Optional[List[Dict[str, Any]]] = Field(None, description="Chart configuration data")
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    confidence_score: Annotated[float, Field(ge=0.0, le=1.0, description="Overall confidence")]
    processing_time_ms: int = Field(description="Processing time in milliseconds")
    data_sources_used: List[DataSource] = Field(description="Data sources utilized")
    created_at: datetime = Field(description="Response timestamp")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }


class PredictiveModel(BaseModel):
    """Schema for predictive analytics models and forecasts."""
    model_name: str = Field(description="Name of the predictive model")
    model_type: str = Field(description="Type of prediction model")
    target_metric: str = Field(description="Metric being predicted")
    forecast_horizon: int = Field(description="Forecast horizon in days")
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    accuracy_score: Optional[Annotated[float, Field(ge=0.0, le=1.0)]] = None
    last_updated: datetime = Field(description="Last metrics update timestamp")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }


class AIAgentInteraction(BaseModel):
    """Schema for AI agent conversation interactions."""
    interaction_id: str = Field(description="Unique interaction identifier")
    user_id: int = Field(description="User ID")
    session_id: str = Field(description="Conversation session ID")
    
    # Message content
    user_message: Annotated[str, Field(max_length=2000, description="User's message")]
    ai_response: Annotated[str, Field(max_length=5000, description="AI agent's response")]
    
    # Context and metadata
    conversation_context: Optional[Dict[str, Any]] = Field(None, description="Conversation context")
    intent_detected: Optional[str] = Field(None, description="Detected user intent")
    entities_extracted: Optional[List[Dict[str, Any]]] = Field(None, description="Extracted entities")
    
    # AI processing details
    ai_model_used: AIModel = Field(description="AI model used for response")
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    confidence_score: Annotated[float, Field(ge=0.0, le=1.0, description="Response confidence")]
    processing_time_ms: int = Field(description="Processing time")
    
    # Engagement metrics
    # Corrected: Use Annotated[int, Field(ge=1, le=5)]
    user_satisfaction: Optional[Annotated[int, Field(ge=1, le=5)]] = None
    follow_up_needed: bool = Field(default=False, description="Follow-up required")
    action_triggered: Optional[str] = Field(None, description="Action triggered by interaction")
    
    created_at: datetime = Field(description="Interaction timestamp")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }


class RealtimeAlert(BaseModel):
    """Schema for real-time alerts and notifications."""
    alert_id: str = Field(description="Unique alert identifier")
    user_id: int = Field(description="Target user ID")
    
    # Alert details
    title: Annotated[str, Field(max_length=200, description="Alert title")]
    message: Annotated[str, Field(max_length=1000, description="Alert message")]
    alert_type: InsightType = Field(description="Type of alert")
    severity: InsightSeverity = Field(description="Alert severity")
    
    # Triggering conditions
    trigger_metric: str = Field(description="Metric that triggered the alert")
    trigger_value: Union[float, int] = Field(description="Value that triggered alert")
    threshold: Union[float, int] = Field(description="Alert threshold")
    condition: Annotated[str, Field(
        pattern="^(above|below|equals|not_equals|percentage_change)$",
        description="Trigger condition"
    )]
    
    # Alert metadata
    data_source: DataSource = Field(description="Data source for the alert")
    time_window: str = Field(description="Time window for alert evaluation")
    frequency: Annotated[str, Field(
        pattern="^(once|hourly|daily|weekly)$",
        description="Alert frequency"
    )]
    
    # Status and actions
    is_acknowledged: bool = Field(default=False, description="Alert acknowledged by user")
    acknowledged_at: Optional[datetime] = Field(None, description="Acknowledgment timestamp")
    action_taken: Optional[str] = Field(None, description="Action taken in response")
    
    created_at: datetime = Field(description="Alert creation timestamp")
    expires_at: Optional[datetime] = Field(None, description="Alert expiration")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }


class FinancialRecommendation(BaseModel):
    """Schema for AI-generated financial recommendations."""
    recommendation_id: str = Field(description="Unique recommendation identifier")
    title: Annotated[str, Field(max_length=200, description="Recommendation title")]
    description: Annotated[str, Field(max_length=2000, description="Detailed description")]
    
    # Recommendation details
    category: str = Field(description="Recommendation category")
    priority: InsightSeverity = Field(description="Priority level")
    potential_impact: str = Field(description="Expected business impact")
    implementation_effort: Annotated[str, Field(
        pattern="^(low|medium|high)$",
        description="Implementation effort required"
    )]
    
    # Financial projections
    estimated_savings: Optional[float] = Field(None, description="Estimated cost savings")
    estimated_revenue_increase: Optional[float] = Field(None, description="Estimated revenue increase")
    roi_projection: Optional[float] = Field(None, description="ROI projection")
    payback_period_days: Optional[int] = Field(None, description="Payback period in days")
    
    # Implementation details
    steps_to_implement: List[str] = Field(description="Implementation steps")
    prerequisites: Optional[List[str]] = Field(None, description="Prerequisites")
    risks: Optional[List[str]] = Field(None, description="Associated risks")
    success_metrics: List[str] = Field(description="Success measurement criteria")
    
    # AI metadata
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    ai_confidence: Annotated[float, Field(ge=0.0, le=1.0, description="AI confidence in recommendation")]
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    data_quality_score: Annotated[float, Field(ge=0.0, le=1.0, description="Quality of underlying data")]
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    personalization_score: Annotated[float, Field(ge=0.0, le=1.0, description="Personalization relevance")]
    
    created_at: datetime = Field(description="Recommendation creation timestamp")
    valid_until: Optional[datetime] = Field(None, description="Recommendation validity period")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }
    trained: datetime = Field(description="Last training timestamp")
    
    # Forecast data
    predictions: List[DataPoint] = Field(description="Predicted values")
    confidence_intervals: Optional[List[Dict[str, float]]] = Field(
        None, 
        description="Confidence intervals for predictions"
    )
    
    # Model metadata
    features_used: List[str] = Field(description="Features used in the model")
    training_data_size: int = Field(description="Size of training dataset")
    model_parameters: Optional[Dict[str, Any]] = Field(None, description="Model hyperparameters")


class MLModelPerformance(BaseModel):
    """Schema for ML model performance metrics."""
    model_name: str = Field(description="Model identifier")
    model_version: str = Field(description="Model version")
    
    # Performance metrics
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    accuracy: Annotated[float, Field(ge=0.0, le=1.0, description="Model accuracy")]
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    precision: Annotated[float, Field(ge=0.0, le=1.0, description="Model precision")]
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    recall: Annotated[float, Field(ge=0.0, le=1.0, description="Model recall")]
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    f1_score: Annotated[float, Field(ge=0.0, le=1.0, description="F1 score")]
    
    # Training details
    training_samples: int = Field(description="Number of training samples")
    validation_samples: int = Field(description="Number of validation samples")
    training_duration_minutes: float = Field(description="Training duration")
    
    # Feature importance
    feature_importance: Dict[str, float] = Field(description="Feature importance scores")
    
    # Model metadata
    hyperparameters: Dict[str, Any] = Field(description="Model hyperparameters")
    training_date: datetime = Field(description="Training completion date")
    next_retrain_date: Optional[datetime] = Field(None, description="Next scheduled retraining")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }


class DataQualityReport(BaseModel):
    """Schema for data quality assessment reports."""
    report_id: str = Field(description="Unique report identifier")
    data_source: DataSource = Field(description="Assessed data source")
    
    # Quality metrics
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    completeness_score: Annotated[float, Field(ge=0.0, le=1.0, description="Data completeness score")]
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    accuracy_score: Annotated[float, Field(ge=0.0, le=1.0, description="Data accuracy score")]
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    consistency_score: Annotated[float, Field(ge=0.0, le=1.0, description="Data consistency score")]
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    timeliness_score: Annotated[float, Field(ge=0.0, le=1.0, description="Data timeliness score")]
    
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    overall_quality_score: Annotated[float, Field(ge=0.0, le=1.0, description="Overall quality score")]
    
    # Quality issues
    missing_data_percentage: float = Field(description="Percentage of missing data")
    duplicate_records: int = Field(description="Number of duplicate records")
    outliers_detected: int = Field(description="Number of outliers detected")
    
    # Recommendations
    quality_issues: List[str] = Field(description="Identified quality issues")
    improvement_recommendations: List[str] = Field(description="Quality improvement recommendations")
    
    assessment_date: datetime = Field(description="Assessment timestamp")
    records_analyzed: int = Field(description="Total records analyzed")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }


# API Response wrapper schemas
class InsightResponse(BaseModel):
    """Standard insight API response wrapper."""
    success: bool = True
    message: str = "Operation completed successfully"
    data: Optional[InsightInDB] = None
    
    class Config:
        schema_extra = {
            "example": {
                "success": True,
                "message": "Insight retrieved successfully",
                "data": {
                    "id": 1,
                    "title": "Revenue Growth Opportunity",
                    "summary": "Identified 15% revenue growth potential",
                    "insight_type": "revenue_trend",
                    "severity": "medium"
                }
            }
        }

class InsightsList(BaseModel):
    """Schema for paginated insights list."""
    insights: List[InsightInDB]
    total: int = Field(description="Total number of insights")
    page: int = Field(description="Current page number")
    per_page: int = Field(description="Items per page")
    pages: int = Field(description="Total number of pages")
    filters_applied: Optional[Dict[str, Any]] = Field(None, description="Applied filters")

class InsightsListResponse(BaseModel):
    """Standard insights list API response wrapper."""
    success: bool = True
    message: str = "Insights retrieved successfully"
    data: Optional[InsightsList] = None
    
    class Config:
        schema_extra = {
            "example": {
                "success": True,
                "message": "Insights retrieved successfully",
                "data": {
                    "insights": [],
                    "total": 0,
                    "page": 1,
                    "per_page": 20,
                    "pages": 0
                }
            }
        }


class AnalyticsQueryResponse(BaseModel):
    """Response wrapper for analytics queries."""
    success: bool = True
    message: str = "Analytics query processed successfully"
    data: Optional[AnalyticsResponse] = None
    
    class Config:
        schema_extra = {
            "example": {
                "success": True,
                "message": "Analytics query processed successfully",
                "data": {
                    "query_id": "q_123456",
                    "original_query": "What was my revenue last month?",
                    "ai_response": "Your revenue last month was $25,000...",
                    "confidence_score": 0.95
                }
            }
        }

class DashboardMetrics(BaseModel):
    """Schema for dashboard metrics summary."""
    total_insights: int = Field(description="Total insights generated")
    unread_insights: int = Field(description="Unread insights count")
    critical_alerts: int = Field(description="Critical alerts count")
    
    # Financial health indicators
    # Corrected: Use Annotated[float, Field(ge=0.0, le=100.0)]
    financial_health_score: Annotated[float, Field(ge=0.0, le=100.0, description="Overall financial health")]
    cash_flow_status: Annotated[str, Field(
        pattern="^(positive|negative|stable|declining)$",
        description="Cash flow status"
    )]
    revenue_trend: Annotated[str, Field(
        pattern="^(growing|declining|stable)$",
        description="Revenue trend"
    )]
    
    # Key metrics
    key_metrics: List[MetricValue] = Field(description="Key business metrics")
    recent_insights: List[InsightInDB] = Field(description="Recent insights preview")
    trending_topics: List[str] = Field(description="Trending insight topics")

    

class DashboardResponse(BaseModel):
    """Dashboard metrics API response wrapper."""
    success: bool = True
    message: str = "Dashboard metrics retrieved successfully"
    data: Optional[DashboardMetrics] = None
    
    class Config:
        schema_extra = {
            "example": {
                "success": True,
                "message": "Dashboard metrics retrieved successfully",
                "data": {
                    "total_insights": 45,
                    "unread_insights": 12,
                    "critical_alerts": 2,
                    "financial_health_score": 78.5
                }
            }
        }


class RecommendationsResponse(BaseModel):
    """Financial recommendations API response wrapper."""
    success: bool = True
    message: str = "Recommendations retrieved successfully"
    data: Optional[List[FinancialRecommendation]] = None
    
    class Config:
        schema_extra = {
            "example": {
                "success": True,
                "message": "Recommendations retrieved successfully",
                "data": [
                    {
                        "recommendation_id": "rec_123",
                        "title": "Optimize Payment Processing",
                        "category": "cost_optimization",
                        "priority": "medium",
                        "estimated_savings": 500.0
                    }
                ]
            }
        }
    trained: datetime = Field(description="Last training timestamp")
    
    # Forecast data
    predictions: List[DataPoint] = Field(description="Predicted values")
    confidence_intervals: Optional[List[Dict[str, float]]] = Field(
        None, 
        description="Confidence intervals for predictions"
    )
    
    # Model metadata
    features_used: List[str] = Field(description="Features used in the model")
    training_data_size: int = Field(description="Size of training dataset")
    model_parameters: Optional[Dict[str, Any]] = Field(None, description="Model hyperparameters")


class RiskAssessment(BaseModel):
    """Schema for AI-powered risk assessment."""
    assessment_id: str = Field(description="Unique assessment identifier")
    # Corrected: Use Annotated[float, Field(ge=0.0, le=100.0)]
    risk_score: Annotated[float, Field(ge=0.0, le=100.0, description="Overall risk score (0-100)")]
    risk_level: Annotated[str, Field(
        pattern="^(very_low|low|medium|high|very_high)$",
        description="Risk level classification"
    )]
    risk_factors: List[Dict[str, Any]] = Field(description="Identified risk factors")
    mitigation_strategies: List[str] = Field(description="Recommended mitigation strategies")
    
    # Risk categories
    # Corrected: Use Annotated[float, Field(ge=0.0, le=100.0)]
    financial_risk: Annotated[float, Field(ge=0.0, le=100.0, description="Financial risk score")]
    # Corrected: Use Annotated[float, Field(ge=0.0, le=100.0)]
    operational_risk: Annotated[float, Field(ge=0.0, le=100.0, description="Operational risk score")]
    # Corrected: Use Annotated[float, Field(ge=0.0, le=100.0)]
    market_risk: Annotated[float, Field(ge=0.0, le=100.0, description="Market risk score")]
    # Corrected: Use Annotated[float, Field(ge=0.0, le=100.0)]
    compliance_risk: Annotated[float, Field(ge=0.0, le=100.0, description="Compliance risk score")]
    
    assessed_at: datetime = Field(description="Assessment timestamp")
    valid_until: datetime = Field(description="Assessment validity period")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }


class BenchmarkComparison(BaseModel):
    """Schema for industry benchmark comparisons."""
    metric_name: str = Field(description="Benchmarked metric")
    user_value: Union[float, int] = Field(description="User's metric value")
    industry_average: Union[float, int] = Field(description="Industry average")
    industry_median: Union[float, int] = Field(description="Industry median")
    # Corrected: Use Annotated[float, Field(ge=0.0, le=100.0)]
    percentile_rank: Annotated[float, Field(ge=0.0, le=100.0, description="User's percentile rank")]
    
    performance_level: Annotated[str, Field(
        pattern="^(poor|below_average|average|above_average|excellent)$",
        description="Performance level classification"
    )]
    
    benchmark_data: Dict[str, Any] = Field(description="Detailed benchmark data")
    comparison_date: date = Field(description="Benchmark comparison date")
    data_quality: Annotated[str, Field(
        pattern="^(high|medium|low)$", 
        description="Quality of benchmark data"
    )]


class InsightFilters(BaseModel):
    """Schema for filtering insights."""
    insight_types: Optional[List[InsightType]] = None
    categories: Optional[List[InsightCategory]] = None
    severity_levels: Optional[List[InsightSeverity]] = None
    date_from: Optional[date] = None
    date_to: Optional[date] = None
    is_read: Optional[bool] = None
    is_archived: Optional[bool] = None
    # Corrected: Use Annotated[float, Field(ge=0.0, le=1.0)]
    min_confidence: Optional[Annotated[float, Field(ge=0.0, le=1.0)]] = None
    data_sources: Optional[List[DataSource]] = None
    search_text: Optional[Annotated[str, Field(max_length=200)]] = None